{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/0e/a91780d07592b1abf9c91344ce459472cc19db3b67fdf3a61dca6ebb2f5c/tensorflow_hub-0.7.0-py2.py3-none-any.whl (89kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (3.9.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow_hub) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (1.17.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow_hub) (41.2.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.7.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting bert-for-tf2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/b4/1a3da73498960866ad0510ead86b133569ff012bf1c77d82ce95203779fc/bert-for-tf2-0.13.2.tar.gz (40kB)\n",
      "\u001b[K     |████████████████████████████████| 40kB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting py-params>=0.7.3 (from bert-for-tf2)\n",
      "  Downloading https://files.pythonhosted.org/packages/ec/17/71c5f3c0ab511de96059358bcc5e00891a804cd4049021e5fa80540f201a/py-params-0.8.2.tar.gz\n",
      "Collecting params-flow>=0.7.1 (from bert-for-tf2)\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/12/2604f88932f285a473015a5adabf08496d88dad0f9c1228fab1547ccc9b5/params-flow-0.7.4.tar.gz\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.7.1->bert-for-tf2) (1.17.2)\n",
      "Collecting tqdm (from params-flow>=0.7.1->bert-for-tf2)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/c9/7fc20feac72e79032a7c8138fd0d395dc6d8812b5b9edf53c3afd0b31017/tqdm-4.41.1-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 2.6MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
      "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.13.2-cp36-none-any.whl size=34685 sha256=a295f9195080a257e85d91d56d175b073f99c2a8e20c9e89290c36641ff59c55\n",
      "  Stored in directory: /root/.cache/pip/wheels/d8/e1/95/7fa0b466d35f4280a8842a6653f9cd37f89e83832770daf85f\n",
      "  Building wheel for py-params (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py-params: filename=py_params-0.8.2-cp36-none-any.whl size=6481 sha256=cf959c39ef20b8969b5a305682e1f241773f4c68bfdd68dd98912be02a1ee22a\n",
      "  Stored in directory: /root/.cache/pip/wheels/83/3a/9c/baf35d6f17f0c2c6b61bf8ac3ab9fc12df0e41432ccaeecacb\n",
      "  Building wheel for params-flow (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for params-flow: filename=params_flow-0.7.4-cp36-none-any.whl size=19235 sha256=55a6459d7e5d6ede4f983a3475556104cba1f223d28db446ee832c5c522b8983\n",
      "  Stored in directory: /root/.cache/pip/wheels/86/30/40/507b60d68b67ac87f35e95c98f5b296a32f146d5ae1d1d5aa7\n",
      "Successfully built bert-for-tf2 py-params params-flow\n",
      "Installing collected packages: py-params, tqdm, params-flow, bert-for-tf2\n",
      "Successfully installed bert-for-tf2-0.13.2 params-flow-0.7.4 py-params-0.8.2 tqdm-4.41.1\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 1.7MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.85\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub\n",
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/counts_1grams.txt') as fopen:\n",
    "    f = fopen.read().split('\\n')[:-1]\n",
    "    \n",
    "words = {}\n",
    "for l in f:\n",
    "    w, c = l.split('\\t')\n",
    "    c = int(c)\n",
    "    words[w] = c + words.get(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class SpellCorrector:\n",
    "    \"\"\"\n",
    "    The SpellCorrector extends the functionality of the Peter Norvig's\n",
    "    spell-corrector in http://norvig.com/spell-correct.html\n",
    "    \"\"\"\n",
    "    REGEX_TOKEN = re.compile(r'\\b[a-z]{2,}\\b')\n",
    "\n",
    "    def __init__(self, words):\n",
    "        \"\"\"\n",
    "        :param corpus: the statistics from which corpus to use for the spell correction.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.WORDS = words\n",
    "        self.N = sum(self.WORDS.values())\n",
    "              \n",
    "    @staticmethod\n",
    "    def tokens(text):\n",
    "        return REGEX_TOKEN.findall(text.lower())\n",
    "\n",
    "    def P(self, word):\n",
    "        \"\"\"\n",
    "        Probability of `word`.\n",
    "        \"\"\"\n",
    "        return self.WORDS[word] / self.N\n",
    "\n",
    "    def most_probable(self, words):\n",
    "        _known = self.known(words)\n",
    "        if _known:\n",
    "            return max(_known, key=self.P)\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    @staticmethod\n",
    "    def edit_step(word):\n",
    "        \"\"\"\n",
    "        All edits that are one edit away from `word`.\n",
    "        \"\"\"\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(self, word):\n",
    "        \"\"\"\n",
    "        All edits that are two edits away from `word`.\n",
    "        \"\"\"\n",
    "        return (e2 for e1 in self.edit_step(word)\n",
    "                for e2 in self.edit_step(e1))\n",
    "\n",
    "    def known(self, words):\n",
    "        \"\"\"\n",
    "        The subset of `words` that appear in the dictionary of WORDS.\n",
    "        \"\"\"\n",
    "        return set(w for w in words if w in self.WORDS)\n",
    "\n",
    "    def edit_candidates(self, word, assume_wrong=False, fast=True):\n",
    "        \"\"\"\n",
    "        Generate possible spelling corrections for word.\n",
    "        \"\"\"\n",
    "\n",
    "        if fast:\n",
    "            ttt = self.known(self.edit_step(word)) or {word}\n",
    "        else:\n",
    "            ttt = self.known(self.edit_step(word)) or self.known(self.edits2(word)) or {word}\n",
    "        \n",
    "        ttt = self.known([word]) | ttt\n",
    "        return list(ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrector = SpellCorrector(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wife', 'rife', 'life', 'gift', 'give', 'gibe']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#possible_states = corrector.edit_candidates('eting')\n",
    "possible_states = corrector.edit_candidates('gife')\n",
    "possible_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**mask** me something to eat'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text = 'scientist suggests eting burger can lead to obesity'\n",
    "text = 'gife me something to eat'\n",
    "#text_mask = text.replace('eting', '**mask**')\n",
    "text_mask = text.replace('gife', '**mask**')\n",
    "text_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128  # Your choice here.\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See BERT paper: https://arxiv.org/pdf/1810.04805.pdf\n",
    "# And BERT implementation convert_single_example() at https://github.com/google-research/bert/blob/master/run_classifier.py\n",
    "\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wife me something to eat',\n",
       " 'rife me something to eat',\n",
       " 'life me something to eat',\n",
       " 'gift me something to eat',\n",
       " 'give me something to eat',\n",
       " 'gibe me something to eat']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced_masks = [text_mask.replace('**mask**', state) for state in possible_states]\n",
    "replaced_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in replaced_masks:\n",
    "    candidat_inputs = list()\n",
    "    tokens = tokenizer.tokenize(sent)\n",
    "    stokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "    input_masks = get_masks(stokens, max_seq_length)\n",
    "    input_segments = get_segments(stokens, max_seq_length)\n",
    "    candidat_inputs.append([[input_ids], [input_masks], [input_segments]])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.01106459  0.3070191  -0.00853465 ... -0.3068895   0.00639752\n",
      "    0.27061567]\n",
      "  [ 0.18352532 -0.5772168   0.7146587  ... -0.11176492  0.6690078\n",
      "   -0.02929941]\n",
      "  [-0.12705982 -0.5298172   1.2385045  ...  0.22001466 -0.09279799\n",
      "    0.24640633]\n",
      "  ...\n",
      "  [ 0.27750766  0.28496078  0.53381324 ...  0.21475478 -0.30043748\n",
      "    0.00225102]\n",
      "  [ 0.26203728  0.22347955  0.64350784 ...  0.17977902 -0.2661268\n",
      "   -0.08173329]\n",
      "  [ 0.28714928  0.14969665  0.68946457 ...  0.14365995 -0.27989298\n",
      "   -0.14007063]]]\n"
     ]
    }
   ],
   "source": [
    "for inp in candidat_inputs:\n",
    "    pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])\n",
    "    print(all_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
